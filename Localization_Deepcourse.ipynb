{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H9Ftyf1ObxuQ"
   },
   "source": [
    "<center><h1>Localization</h1></center>\n",
    "\n",
    "<center><h2><a href=\"https://deepcourse-epita.netlify.app/\">Course link</a></h2></center>\n",
    "\n",
    "To keep your modifications in case you want to come back later to this colab, do *File -> Save a copy in Drive*.\n",
    "\n",
    "If you find a mistake, or know how to improve this notebook, please open an issue [here](https://gitlab.com/ey_datakalab/course_epita)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TOYobYxhgz0U"
   },
   "outputs": [],
   "source": [
    "!wget https://deepcourse-epita.netlify.app/code/loc/imagenet.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "14Prd8OelX28"
   },
   "outputs": [],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Cya3fs4Kb4Pc"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision\n",
    "from PIL import Image\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YtSvQGhq40A"
   },
   "source": [
    "In this colab, we are going to **localize** objects in an image, without even training a model.\n",
    "\n",
    "Or to be more correct, with only taking a model pretrained on classification.\n",
    "\n",
    "Let's load our resnet pretrained on ImageNet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C0Rt3NjBb8LA"
   },
   "outputs": [],
   "source": [
    "net = torchvision.models.resnet50(pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kuJsWekfrI-Y"
   },
   "source": [
    "We need to modify its forward function. Ideally we should have download the code and modify it. But on colab, it may be a bit unpracticable, so we are going to monkey-patch the forward function.\n",
    "\n",
    "We strip the last global average pooling (`self.avgpool`) and the classifier (`self.fc`) and we add a new layer that we call `self.conv1x1`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7g8eEPB6cDqs"
   },
   "outputs": [],
   "source": [
    "def forward(self, x):\n",
    "  x = self.conv1(x)\n",
    "  x = self.bn1(x)\n",
    "  x = self.relu(x)\n",
    "  x = self.maxpool(x)\n",
    "\n",
    "  x = self.layer1(x)\n",
    "  x = self.layer2(x)\n",
    "  x = self.layer3(x)\n",
    "  x = self.layer4(x)\n",
    "\n",
    "  # We remove those two layers...\n",
    "  # x = self.avgpool(x)\n",
    "  # x = self.fc(x)\n",
    "\n",
    "  # ... and we add this layer:\n",
    "  x = self.conv1x1(x)\n",
    "\n",
    "  return x\n",
    "\n",
    "\n",
    "net.forward = forward.__get__(\n",
    "    net,\n",
    "    torchvision.models.ResNet\n",
    ");  # monkey-patching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emdNdK74rq3I"
   },
   "source": [
    "We are going to exploit an important fact to do our localization for free.\n",
    "\n",
    "Convolution with 1x1 kernel, also called **pointwise convolutions**, are actually a fully-connected layer applied independently on every pixels.\n",
    "\n",
    "We need the spatial dimension, that we loose with global pooling and fully-connected, to do localization. Therefore we are going to convert a fully-connected layer in a 1x1 convolution.\n",
    "\n",
    "Inspect the weights and bias shapes of both layers, and try to find a way to transfer the parameters learned by one to the other:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cqy8d9yUccX8"
   },
   "outputs": [],
   "source": [
    "print(\"fc weight and bias:\", net.fc.weight.data.shape, net.fc.bias.data.shape)\n",
    "\n",
    "conv1x1 = nn.Conv2d(2048, 1000, kernel_size=1)\n",
    "\n",
    "print(\"conv1x1 weight and bias:\", conv1x1.weight.data.shape, conv1x1.bias.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WpNLRrUYgzTg"
   },
   "outputs": [],
   "source": [
    "# Execute this cell to see the solution, but try to do it by yourself before!\n",
    "!wget https://deepcourse-epita.netlify.app/code/loc/conv.py\n",
    "%pycat conv.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TZFtD-2sw7g"
   },
   "source": [
    "And don't forget to provide this new conv to the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "37wHDcULdeqI"
   },
   "outputs": [],
   "source": [
    "net.conv1x1 = conv1x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-X7P00Dps0ts"
   },
   "source": [
    "Just to be sure, check the output dimension:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k3ol1mQPdgqQ"
   },
   "outputs": [],
   "source": [
    "net(torch.randn(1, 3, 224, 224)).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7gzlcCq_s5Ig"
   },
   "source": [
    "Now we want to localize a class in particular. Let's say the class \"dog\". You can take an other class if you want, I only use this one because it is the most frequent class of ImageNet.\n",
    "\n",
    "Among ImageNet's 1000 classes, there are hundred of dog species.\n",
    "\n",
    "Luckily, ImageNet's classes are based on the hierarchy of **wordnet**. Therefore we can find classes belonging to \"dog\".\n",
    "\n",
    "First, we load wordnet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t9wKjp6zdlB8"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wUkPiOmotc04"
   },
   "source": [
    "Then, we load the classes of ImageNet and their id (**synset**) in the wordnet hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rKAB8eBHeKYk"
   },
   "outputs": [],
   "source": [
    "with open(\"imagenet.json\") as f:\n",
    "  imagenet_labels = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2QdwZsLqtgAA"
   },
   "outputs": [],
   "source": [
    "!head imagenet.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qeot4nQEtvdE"
   },
   "source": [
    "We aggregate all dog classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SBGkPrBYeNEc"
   },
   "outputs": [],
   "source": [
    "class_to_localize = \"dog\"\n",
    "nltk.download('omw-1.4')\n",
    "parent_name = wn.synsets(class_to_localize)[0]._name\n",
    "print(f\"Parent is <{parent_name}>\")\n",
    "\n",
    "indexes = set()\n",
    "names = set()\n",
    "\n",
    "for index, metadata in imagenet_labels.items():\n",
    "  base_synset = wn.synset_from_pos_and_offset(\"n\", int(metadata[\"id\"].split('-')[0]))\n",
    "\n",
    "  synset = base_synset\n",
    "  while synset._name != parent_name:\n",
    "    parents = synset.hypernyms()\n",
    "    if len(parents) == 0:  # no more parents, we are at the root\n",
    "      break\n",
    "\n",
    "    synset = parents[0]\n",
    "\n",
    "  if synset._name == parent_name:\n",
    "    indexes.add(int(index))\n",
    "    names.add(base_synset._name)\n",
    "\n",
    "indexes = torch.tensor(list(indexes))\n",
    "print(f\"There are {len(indexes)} classes\")\n",
    "list(names)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OleJFeGWt_X0"
   },
   "source": [
    "Let's try our model on an image with two dogs (but you can use any image you want):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OzCY-KSbuLzI"
   },
   "outputs": [],
   "source": [
    "!wget https://deepcourse-epita.netlify.app/code/loc/2dogs.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IV0MYPR_eoRE"
   },
   "outputs": [],
   "source": [
    "image = Image.open(\"2dogs.jpg\")\n",
    "image.thumbnail((512, 512), Image.ANTIALIAS)\n",
    "image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xkw25QgMuP4E"
   },
   "source": [
    "We still need to preprocess our imagenet in the same way the model was trained: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMegeaIye5gM"
   },
   "outputs": [],
   "source": [
    "imagenet_mean = torch.tensor([0.485, 0.456, 0.406])\n",
    "imagenet_std = torch.tensor([0.229, 0.224, 0.225])\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(imagenet_mean, imagenet_std),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lE1oo1d4vNTU"
   },
   "source": [
    "Now code a method to get the *attention* of the model which will be our localization.\n",
    "\n",
    "Here are the steps:\n",
    "\n",
    "1. resize image to the asked dimensions\n",
    "2. preprocess the image\n",
    "3. extract the spatial logits\n",
    "4. use softmax alongside the correct dimension\n",
    "5. only keep the channels in `indexes` that we computed from wordnet and sum them\n",
    "\n",
    "\n",
    "*Pro-tip*: to avoid storing intermediary activations that are useful only when doing backward, use the context manager `torch.no_grad()`:\n",
    "\n",
    "```python\n",
    "with torch.no_grad():\n",
    "  y = net(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lCSzRSkEmUQk"
   },
   "outputs": [],
   "source": [
    "def generate_attention(path, size=(224, 224)):\n",
    "  image = Image.open(path)\n",
    "  image.thumbnail(size, Image.ANTIALIAS)\n",
    "\n",
    "  # TODO\n",
    "\n",
    "  return image, attention_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4betnXvwFYY"
   },
   "outputs": [],
   "source": [
    "# Execute this cell to see the solution, but try to do it by yourself before!\n",
    "!wget https://deepcourse-epita.netlify.app/code/loc/attn.py\n",
    "%pycat attn.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qlyYQA33wJ-I"
   },
   "source": [
    "Let's try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rSXzK5cHfAHo"
   },
   "outputs": [],
   "source": [
    "image, attn = generate_attention(\"2dogs.jpg\", (224, 224))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.axis(\"off\")\n",
    "plt.imshow(image)\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.axis(\"off\")\n",
    "plt.imshow(attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LWza8dczwLm0"
   },
   "source": [
    "Hum... Not that great right? I mean we knew it won't be very precise, but this is super bad.\n",
    "\n",
    "But what if the resolution was larger?..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahCKpnbPj1nc"
   },
   "outputs": [],
   "source": [
    "image, attn = generate_attention(\"2dogs.jpg\", (512, 512))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.axis(\"off\")\n",
    "plt.imshow(image)\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.axis(\"off\")\n",
    "plt.imshow(attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HAATKU2BwV-4"
   },
   "source": [
    "larger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kr3cBh9Qj6mM"
   },
   "outputs": [],
   "source": [
    "image, attn = generate_attention(\"2dogs.jpg\", (1024, 1024))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.axis(\"off\")\n",
    "plt.imshow(image)\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.axis(\"off\")\n",
    "plt.imshow(attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cp91XPV3wZ0M"
   },
   "source": [
    "larger?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NA_4z-7CkGGI"
   },
   "outputs": [],
   "source": [
    "image, attn = generate_attention(\"2dogs.jpg\", (1500, 1500))\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.axis(\"off\")\n",
    "plt.imshow(image)\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.axis(\"off\")\n",
    "plt.imshow(attn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AC6cJULlwceg"
   },
   "source": [
    "You should see more precisely the shapes of the dogs as the resolution (and computational cost) increases.\n",
    "\n",
    "But as the resolution is larger, there are also more artefacts where the model thinks it has found a dog somewhere in the background.\n",
    "\n",
    "A solution, which is essential to even modern network in segmentation, is to exploit **multiple scales**.\n",
    "\n",
    "Compute the attention with the same image resized at different dimensions, and combine all those attentions together. There can be different aggregation method although I recommend a [geometric mean](https://en.wikipedia.org/wiki/Geometric_mean)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mDDcDbrIkmRg"
   },
   "outputs": [],
   "source": [
    "final_attn = # TODO\n",
    "\n",
    "sizes = (112, 224, 512, 1024, 1500)\n",
    "for size in sizes:\n",
    "  image, attn = generate_attention(\"2dogs.jpg\", (size, size))\n",
    "  resized_attn = F.interpolate(attn[None, None], (32, 47))[0, 0]\n",
    "  # TODO\n",
    "\n",
    "final_attn = # TODO\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.axis(\"off\")\n",
    "plt.imshow(image)\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.axis(\"off\")\n",
    "plt.imshow(final_attn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dslZ0Uoxqw5w"
   },
   "outputs": [],
   "source": [
    "# Execute this cell to see the solution, but try to do it by yourself before!\n",
    "!wget https://deepcourse-epita.netlify.app/code/loc/geom.py\n",
    "%pycat geom.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fHsrX2-UxrEM"
   },
   "source": [
    "It should be better than previous results.\n",
    "\n",
    "Of course, we don't have a super precise segmentation of the classes with this method. But we managed to do some crude localization without any training.\n",
    "\n",
    "If we wanted to improve results, we could finetune this 1x1 convolution using actual segmentation data. This is more or less what **Fully Convolutional Network** published at CVPR 2015 did! Read about the paper [here](https://arxiv.org/pdf/1411.4038.pdf), and if you're feeling courageous, implement it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjP7M9Ajx-_M"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Localization - Deepcourse",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
